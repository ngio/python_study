"""
ì…ë ¥ëœ ì›¹ í˜ì´ì§€ ë‚´ì—ì„œ ëª¨ë“  <img> ìš”ì†Œë¥¼ ì¶”ì¶œí•˜ëŠ” í”„ë¡œê·¸ë¨ì„ **requests**ì™€ BeautifulSoup ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ğŸ”

ì´ ë°©ë²•ì€ ì›¹ í¬ë¡¤ë§(Web Crawling)ì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì´ë©°, HTMLì„ íŒŒì‹±(Parsing)í•˜ì—¬ ì›í•˜ëŠ” íŠ¹ì • íƒœê·¸ë¥¼ ì‰½ê²Œ ì°¾ì•„ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import sys

def extract_images_from_url(url):
    """
    ì£¼ì–´ì§„ URLì˜ ì›¹ í˜ì´ì§€ì—ì„œ ëª¨ë“  <img> íƒœê·¸ì˜ src ì†ì„±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    :param url: ë¶„ì„í•  ì›¹ í˜ì´ì§€ì˜ URL
    :return: ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
    """
    if not (url.startswith('http://') or url.startswith('https://')):
        # ì‚¬ìš©ìê°€ í”„ë¡œí† ì½œì„ ìƒëµí–ˆì„ ê²½ìš° https://ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì¶”ê°€
        url = 'https://' + url
        
    image_list = []
    
    print(f"URLì— ì ‘ì† ì¤‘: {url}")
    
    try:
        # 1. HTTP ìš”ì²­ ë³´ë‚´ê¸°
        # User-Agentë¥¼ ì„¤ì •í•˜ì—¬ ë´‡ ì ‘ê·¼ì´ ì•„ë‹˜ì„ ì•Œë¦¬ê³  ì ‘ì† ê±°ë¶€ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë°œìƒ
        
        # 2. HTML íŒŒì‹± (BeautifulSoup ì‚¬ìš©)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 3. ëª¨ë“  <img> íƒœê·¸ ì°¾ê¸°
        img_tags = soup.find_all('img')
        
        # 4. ê° íƒœê·¸ì—ì„œ src ì†ì„± ì¶”ì¶œ
        for img in img_tags:
            src = img.get('src')
            if src:
                # 5. ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                # <img src="/images/logo.png">ì™€ ê°™ì€ ìƒëŒ€ ê²½ë¡œë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.
                absolute_url = urljoin(url, src)
                image_list.append(absolute_url)
                
    except requests.exceptions.RequestException as e:
        print(f"\n[ì˜¤ë¥˜] ì›¹ í˜ì´ì§€ì— ì ‘ì†í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None
        
    return image_list

if __name__ == "__main__":
    
    # 1. ì‚¬ìš©ìë¡œë¶€í„° URL ì…ë ¥ ë°›ê¸°
    target_url = input("ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ì›¹ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: google.com): ").strip()
    
    if not target_url:
        print("URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit()

    # 2. ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤í–‰
    images = extract_images_from_url(target_url)

    # 3. ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    
    if images is not None:
        print(f"ğŸ“Œ ë°œê²¬ëœ ì´ë¯¸ì§€ ìš”ì†Œ ê°œìˆ˜: {len(images)}ê°œ")
        print("--- ì¶”ì¶œëœ ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸ ---")
        
        # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥ (ë„ˆë¬´ ê¸¸ì–´ì§€ëŠ” ê²ƒì„ ë°©ì§€)
        for i, img_url in enumerate(images[:10]):
            print(f"{i+1}. {img_url}")

        if len(images) > 10:
            print(f"...\n(ì´ {len(images)}ê°œì˜ ì´ë¯¸ì§€ URLì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.)")
    
    print("="*50)
